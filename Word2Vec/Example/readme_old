This directory contains data and code to run a simple NLP Word2Vec example along with several Word2Vec references.
Word2Vec NLP example: word2vecNLPExample.py
pretrained word2vec embeddings:
  Wikipedia2Vec data: https://wikipedia2vec.github.io/wikipedia2vec/pretrained/
    enwiki_20180420_100d.txt.bz2 (~16.5 minutes to load) (~18.09 minutes to load and run analogies (~65% accuracy))
  google news dataset: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit
    GoogleNews-vectors-negative300.bin.gz (~3.3 minutes to load) (~10.9 minutes to load and run analogies (~74% accuracy))
pretrained glove embeddings: https://nlp.stanford.edu/projects/glove/ (converted to word2vec and saved)
  w2v_glove.6B.50d.txt (~40 seconds to load) (~2.75 to load and run analogies (~46% accuracy))
  w2v_glove.6B.100d.txt (~1.25 minutes to load) (~3.75 minutes to load and run analogies (~63% accuracy))
  w2v_glove.twitter.27B.25d.txt (~1.09 minutes to load) (~2.5 minutes to load and run analogies (~13% accuracy))
questions-words.txt download: https://github.com/nicholas-leonard/word2vec/blob/master/questions-words.txt
python gensim api reference for Word2Vec: https://radimrehurek.com/gensim/models/word2vec.html
google reference for Word2Vec: https://code.google.com/archive/p/word2vec/
