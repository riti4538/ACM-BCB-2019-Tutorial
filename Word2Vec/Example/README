This directory contains code to run a simple NLP Word2Vec example. Pretrained embeddings may be downloaded from several places.
Word2Vec NLP examplecode:
  word2vecNLPExample.py
Pretrained word2vec embeddings:
  Wikipedia2Vec data: https://wikipedia2vec.github.io/wikipedia2vec/pretrained/
    enwiki_20180420_100d.txt.bz2 (~16.5 minutes to load) (~18.09 minutes to load and run analogies (~65% accuracy))
  google news dataset: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit
    GoogleNews-vectors-negative300.bin.gz (~3.3 minutes to load) (~10.9 minutes to load and run analogies (~74% accuracy))
Pretrained glove embeddings: https://nlp.stanford.edu/projects/glove/ (converted to Word2Vec to test)
  glove.6B.50d.txt (~40 seconds to load) (~2.75 to load and run analogies (~46% accuracy))
  glove.6B.100d.txt (~1.25 minutes to load) (~3.75 minutes to load and run analogies (~63% accuracy))
  glove.twitter.27B.25d.txt (~1.09 minutes to load) (~2.5 minutes to load and run analogies (~13% accuracy))
Analogy test file:
  questions-words.txt download: https://github.com/nicholas-leonard/word2vec/blob/master/questions-words.txt
Word2Vec references:
  python gensim api reference for Word2Vec: https://radimrehurek.com/gensim/models/word2vec.html
  google reference for Word2Vec: https://code.google.com/archive/p/word2vec/

